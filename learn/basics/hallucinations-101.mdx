---
title: Hallucinations 101
description: Learn what AI hallucinations are, how to spot them, and proven techniques to reduce their occurrence in your interactions with AI systems.
---

## What Are AI Hallucinations?

When we talk about AI hallucinations, we're describing a phenomenon where an artificial intelligence model generates information that sounds confident and plausible but is actually incorrect, fabricated, or unsupported by its training data or provided context.

Think of it this way: imagine asking a friend for directions to a campus building. Instead of admitting they don't know, they confidently give you detailed turn-by-turn instructions to a building that doesn't exist. The instructions sound legitimate, but following them leads nowhere. That's essentially what happens when an AI hallucinates.

**Key characteristics of hallucinations:**

- The AI presents false information as fact
- The response appears confident and well-structured
- The fabricated content often blends seamlessly with accurate information
- The AI doesn't indicate uncertainty or lack of knowledge

### Why Do Hallucinations Happen?

AI language models are trained to predict the most likely next word or phrase based on patterns in their training data. They don't "know" facts the way humans do. Instead, they generate responses based on statistical patterns. When the model encounters a question outside its training data or lacks sufficient context, it may still generate a response that follows those patterns—even if the content is invented.

Common causes include:

- Questions about information not in the training data
- Requests for very specific details (like exact statistics or dates)
- Ambiguous queries that could have multiple interpretations
- Topics where the training data contained inconsistencies or errors

## How to Identify Hallucinations

Recognizing when an AI might be hallucinating is a critical skill. Here are the warning signs to watch for:

### Red Flags and Warning Signs

**1. Overly specific details without sources**

If an AI provides exact statistics, dates, or quotes without citing sources, verify them independently. Real data comes from somewhere.

**Example hallucination:**
> "According to a 2023 study by the National Education Foundation, 87.3% of college students prefer asynchronous learning environments."

**2. Inconsistent information**

Watch for contradictions within the same response or across multiple interactions about the same topic.

**3. Suspiciously perfect answers**

If a response seems too good to be true for a complex or obscure question, it might be. Real information often comes with nuance, caveats, or acknowledgment of complexity.

**4. Unfamiliar references**

Citations to papers, books, or sources that don't exist or can't be verified are classic hallucination indicators.

**5. Lack of uncertainty**

Legitimate answers to difficult questions often include phrases like "typically," "in most cases," or "according to available information." Total confidence on every topic is suspicious.

### Practical Verification Steps

When you suspect a hallucination:

1. **Cross-reference with official sources**: For university-related information, check official BYU-Idaho websites, catalogs, or contact relevant departments
2. **Ask follow-up questions**: Request clarification or additional details. Hallucinations often fall apart under scrutiny
3. **Check multiple sources**: Don't rely on a single AI response for important decisions
4. **Verify specific claims**: Look up statistics, dates, quotes, and citations independently

## Techniques to Reduce Hallucinations

While no method eliminates hallucinations entirely, several strategies significantly reduce their occurrence.

### 1. Provide Clear, Specific Context

The more relevant information you provide, the less the AI needs to "fill in the gaps" with generated content.

**Weak prompt:**
> "Tell me about registration."

**Strong prompt:**
> "I'm a sophomore biology major trying to register for Fall 2025 classes. What's the process for adding a lab section after initial registration closes?"

### 2. Ask for Source-Based Responses

Explicitly request that the AI base its response on specific, verifiable information.

**Example:**
> "Based on the official BYU-Idaho academic calendar, when does Spring 2025 semester begin?"

### 3. Request Acknowledgment of Limitations

Encourage the AI to admit when it doesn't have sufficient information.

**Example:**
> "If you're not certain about this policy, please let me know rather than guessing."

### 4. Use Retrieval-Augmented Generation (RAG)

This is where the BYU-Idaho Support Agent truly shines.

**RAG systems ground AI responses in real, verified documents.** Instead of relying solely on training data, RAG-enabled AI systems search through curated knowledge bases—like official BYU-Idaho policies, procedures, and resources—before generating responses.

The process works like this:

1. You ask a question
2. The system searches relevant documents for information
3. The AI generates a response based on those retrieved documents
4. The response is anchored in verified, institutional knowledge

This dramatically reduces hallucinations because the AI is working from current, accurate source material rather than memory or pattern matching.

**Want to learn more?** Check out our detailed guide on [what is RAG](/learn/rag/what-is-rag) to understand how this technology works and why it's effective.

### 5. Break Complex Questions into Smaller Parts

Instead of asking one large, multifaceted question, break it into discrete, manageable pieces. This reduces the chance of the AI making assumptions or filling gaps incorrectly.

**Instead of:**
> "What are all the requirements, deadlines, and procedures for changing my major, adding a minor, and adjusting my course schedule?"

**Try:**
> "What are the requirements for changing my major?"
> "What's the deadline for adding a minor?"
> "How do I adjust my course schedule after registration?"

### 6. Verify Critical Information

For important decisions—registration deadlines, policy changes, financial matters—always verify AI-provided information with official university sources.

**BYU-Idaho Support Contact Information:**

- Phone: [208-496-1411](tel:812084961411)
- Email: ask@byui.edu
- Hours: Monday-Friday 7:00 AM - 8:00 PM; Saturday 11:00 AM - 8:00 PM (Mountain Time)

## Best Practices for Using AI Support Tools

To get the most accurate, helpful responses from the BYU-Idaho Support Agent:

1. **Be specific**: Include relevant details about your situation, major, semester, and question
2. **Ask one question at a time**: This helps the system focus and retrieve the most relevant information
3. **Verify before acting**: Double-check answers to critical questions, especially those involving deadlines or policy
4. **Provide feedback**: If you notice incorrect information, report it so the system can be improved
5. **Use it as a starting point**: Think of AI support as a knowledgeable first resource, not the final authority

## Understanding the Limitations

AI systems, including the BYU-Idaho Support Agent, have limitations:

- They may not have access to the very latest policy updates or changes
- They can't access your personal student records or account-specific information
- They work best with clear, well-defined questions
- They're designed to help, but complex or unique situations may require human staff

When you understand these limitations, you can use AI support tools more effectively and know when to escalate to human support staff.

## The Bottom Line

Hallucinations are a real limitation of current AI technology, but they're manageable. By learning to recognize warning signs, asking better questions, and using systems designed to minimize hallucinations—like RAG-powered tools such as the BYU-Idaho Support Agent—you can confidently leverage AI assistance while staying informed and safe.

The key is balance: trust but verify, use but don't depend entirely, and always keep critical thinking engaged. AI is a powerful tool in your academic toolkit, most effective when used wisely.
